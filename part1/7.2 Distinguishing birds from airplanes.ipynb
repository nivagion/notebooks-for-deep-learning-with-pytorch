{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Distinguishing birds from airplanes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b8244ca530>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Building the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t! Well, why not take a shortcut and just filter the\n",
    "data in cifar10 and remap the labels so they are contiguous? Here’s how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We have a dataset! Next, we need a model to feed our data to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 A fully connected model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                3072,  # <1> input layer 32*32*3\n",
    "                512,   # <2> hidden layer v\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(\n",
    "                512,   # <2> hidden layer ^\n",
    "                n_out, # <3> output layer\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Output of a classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t. We could do something similar here: make our net\u0002work output a single scalar value (so n_out = 1), cast the labels to floats (0.0 for air\u0002plane and 1.0 for bird), and use those as a target for MSELoss (the average of squared\n",
    "differences in the batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in\n",
    "chapter 4, when we have to represent a categorical variable, we should switch to a\n",
    "one-hot-encoding representation of that variable, such as [1, 0] for airplane or [0, 1] <br>for bird (the order is arbitrary). This will still work if we have 10 classes, as in the full\n",
    "CIFAR-10 dataset; we’ll just have a vector of length 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sounds like a tough constraint to enforce in a differentiable way on a vector of num\u0002bers. Yet there’s a very smart trick that does exactly that, and it’s differentiable: it’s\n",
    "called softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 Representing the output as probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we take the elements of the vector, compute the elementwise exponential,\n",
    "and divide each element by the sum of exponentials. In code, it’s something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Softmax requires us to specify\n",
    "the dimension along which the softmax function is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [1.0, 2.0, 3.0]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! We can now add a softmax at the end of our model, and our network\n",
    "will be equipped to produce probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually try running the model before even training it. Let’s do it, just to see\n",
    "what comes out. We first build a batch of one image, our bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to turn our 3 × 32 × 32 image into a 1D tensor and then add an extra dimension in the zeroth position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = img.view(-1).unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4784, 0.5216]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, index = torch.max(out, dim=1)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get\n",
    "training. As in the previous two chapters, we need a loss to minimize during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 A loss for classifying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we want to penalize misclassifications rather than pains\u0002takingly penalize everything that doesn’t look exactly like a 0.0 or 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What we need to maximize in this case is the probability associated with the correct\n",
    "class, out[class_index], where out is the output of softmax and class_index is a vec\u0002tor containing 0 for “airplane” and 1 for “bird” for each sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it’s called negative log likelihood\n",
    "(NLL). It has the expression NLL = - sum(log(out_i[c_i])), where the sum is taken\n",
    "over N samples and c_i is the correct class for sample i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summing up, our loss for classification can be computed as follows. For each sam\u0002ple in the batch:\n",
    "1 Run the forward pass, and obtain the output values from the last (linear) layer.<br>\n",
    "2 Compute their softmax, and obtain probabilities.<br>\n",
    "3 Take the predicted probability corresponding to the correct class (the likeli\u0002hood of the parameters). Note that we know what the correct class is because\n",
    "it’s a supervised problem—it’s our ground truth.<br>\n",
    "4 Compute its logarithm, slap a minus sign in front of it, and add it to the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " use nn.LogSoftmax instead of nn.Softmax, which takes care to make\n",
    "the calculation numerically stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7255, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.6 Training the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re ready to bring back the training loop we wrote in chapter 5 and see\n",
    "how it trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cant run this on laptop, but CAN WITH BATCHING VV DOWN VV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Typically, minibatches are a constant size that we need to set prior to training, just\n",
    "like the learning rate. These are called hyperparameters, to distinguish them from the\n",
    "parameters of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training code, we chose minibatches of size 1 by picking one item at a time from\n",
    "the dataset. The torch.utils.data module has a class that helps with shuffling and\n",
    "organizing the data in minibatches: DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataLoader can be iterated over, so we can use it directly in the inner loop of our\n",
    "new training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.309770\n",
      "Epoch: 1, Loss: 0.401999\n",
      "Epoch: 2, Loss: 0.774094\n",
      "Epoch: 3, Loss: 0.482962\n",
      "Epoch: 4, Loss: 0.569812\n",
      "Epoch: 5, Loss: 0.574610\n",
      "Epoch: 6, Loss: 0.285785\n",
      "Epoch: 7, Loss: 0.410845\n",
      "Epoch: 8, Loss: 0.403018\n",
      "Epoch: 9, Loss: 0.307971\n",
      "Epoch: 10, Loss: 0.423347\n",
      "Epoch: 11, Loss: 0.272704\n",
      "Epoch: 12, Loss: 0.174574\n",
      "Epoch: 13, Loss: 0.261563\n",
      "Epoch: 14, Loss: 0.171392\n",
      "Epoch: 15, Loss: 0.311833\n",
      "Epoch: 16, Loss: 0.305926\n",
      "Epoch: 17, Loss: 0.121198\n",
      "Epoch: 18, Loss: 0.242821\n",
      "Epoch: 19, Loss: 0.276443\n",
      "Epoch: 20, Loss: 0.229399\n",
      "Epoch: 21, Loss: 0.485213\n",
      "Epoch: 22, Loss: 0.099340\n",
      "Epoch: 23, Loss: 0.355324\n",
      "Epoch: 24, Loss: 0.127063\n",
      "Epoch: 25, Loss: 0.313473\n",
      "Epoch: 26, Loss: 0.163736\n",
      "Epoch: 27, Loss: 0.138536\n",
      "Epoch: 28, Loss: 0.202282\n",
      "Epoch: 29, Loss: 0.189369\n",
      "Epoch: 30, Loss: 0.102208\n",
      "Epoch: 31, Loss: 0.083752\n",
      "Epoch: 32, Loss: 0.144667\n",
      "Epoch: 33, Loss: 0.077734\n",
      "Epoch: 34, Loss: 0.270563\n",
      "Epoch: 35, Loss: 0.140062\n",
      "Epoch: 36, Loss: 0.424655\n",
      "Epoch: 37, Loss: 0.187342\n",
      "Epoch: 38, Loss: 0.092957\n",
      "Epoch: 39, Loss: 0.149682\n",
      "Epoch: 40, Loss: 0.157985\n",
      "Epoch: 41, Loss: 0.070291\n",
      "Epoch: 42, Loss: 0.065464\n",
      "Epoch: 43, Loss: 0.060337\n",
      "Epoch: 44, Loss: 0.158627\n",
      "Epoch: 45, Loss: 0.040882\n",
      "Epoch: 46, Loss: 0.166359\n",
      "Epoch: 47, Loss: 0.092274\n",
      "Epoch: 48, Loss: 0.046243\n",
      "Epoch: 49, Loss: 0.056421\n",
      "Epoch: 50, Loss: 0.030675\n",
      "Epoch: 51, Loss: 0.082099\n",
      "Epoch: 52, Loss: 0.036886\n",
      "Epoch: 53, Loss: 0.061186\n",
      "Epoch: 54, Loss: 0.039485\n",
      "Epoch: 55, Loss: 0.061677\n",
      "Epoch: 56, Loss: 0.123529\n",
      "Epoch: 57, Loss: 0.083574\n",
      "Epoch: 58, Loss: 0.044170\n",
      "Epoch: 59, Loss: 0.030438\n",
      "Epoch: 60, Loss: 0.103985\n",
      "Epoch: 61, Loss: 0.137789\n",
      "Epoch: 62, Loss: 0.099522\n",
      "Epoch: 63, Loss: 0.053761\n",
      "Epoch: 64, Loss: 0.062216\n",
      "Epoch: 65, Loss: 0.019569\n",
      "Epoch: 66, Loss: 0.020935\n",
      "Epoch: 67, Loss: 0.033581\n",
      "Epoch: 68, Loss: 0.036713\n",
      "Epoch: 69, Loss: 0.144293\n",
      "Epoch: 70, Loss: 0.029553\n",
      "Epoch: 71, Loss: 0.040880\n",
      "Epoch: 72, Loss: 0.071100\n",
      "Epoch: 73, Loss: 0.072750\n",
      "Epoch: 74, Loss: 0.028022\n",
      "Epoch: 75, Loss: 0.024083\n",
      "Epoch: 76, Loss: 0.030289\n",
      "Epoch: 77, Loss: 0.030168\n",
      "Epoch: 78, Loss: 0.059829\n",
      "Epoch: 79, Loss: 0.015725\n",
      "Epoch: 80, Loss: 0.052135\n",
      "Epoch: 81, Loss: 0.012621\n",
      "Epoch: 82, Loss: 0.046016\n",
      "Epoch: 83, Loss: 0.028002\n",
      "Epoch: 84, Loss: 0.010039\n",
      "Epoch: 85, Loss: 0.014802\n",
      "Epoch: 86, Loss: 0.026987\n",
      "Epoch: 87, Loss: 0.062169\n",
      "Epoch: 88, Loss: 0.050164\n",
      "Epoch: 89, Loss: 0.019409\n",
      "Epoch: 90, Loss: 0.035519\n",
      "Epoch: 91, Loss: 0.019926\n",
      "Epoch: 92, Loss: 0.030055\n",
      "Epoch: 93, Loss: 0.028137\n",
      "Epoch: 94, Loss: 0.013917\n",
      "Epoch: 95, Loss: 0.010945\n",
      "Epoch: 96, Loss: 0.004833\n",
      "Epoch: 97, Loss: 0.005753\n",
      "Epoch: 98, Loss: 0.022048\n",
      "Epoch: 99, Loss: 0.011895\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each inner iteration, imgs is a tensor of size 64 × 3 × 32 × 32—that is, a minibatch of\n",
    "64 (32 × 32) RGB images—while labels is a tensor of size 64 containing label indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.809500\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can certainly add some bling to our model by including more layers, which will\n",
    "increase the model’s depth and capacity. One rather arbitrary possibility is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It is quite common to drop the last nn.LogSoftmax layer from the network and use\n",
    "nn.CrossEntropyLoss as a loss. Let us try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numbers will be exactly the same as with nn.LogSoftmax and nn.NLLLoss.\n",
    "It’s just more convenient to do it all in one pass, with the only gotcha being that the out\u0002put of our model will not be interpretable as probabilities (or log probabilities). We’ll\n",
    "need to explicitly pass the output through a softmax to obtain those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.655991\n",
      "Epoch: 1, Loss: 0.519296\n",
      "Epoch: 2, Loss: 0.582318\n",
      "Epoch: 3, Loss: 0.347640\n",
      "Epoch: 4, Loss: 0.333794\n",
      "Epoch: 5, Loss: 0.453354\n",
      "Epoch: 6, Loss: 0.421042\n",
      "Epoch: 7, Loss: 0.467150\n",
      "Epoch: 8, Loss: 0.462818\n",
      "Epoch: 9, Loss: 0.272433\n",
      "Epoch: 10, Loss: 0.379159\n",
      "Epoch: 11, Loss: 0.414412\n",
      "Epoch: 12, Loss: 0.505951\n",
      "Epoch: 13, Loss: 0.298752\n",
      "Epoch: 14, Loss: 0.263529\n",
      "Epoch: 15, Loss: 0.356218\n",
      "Epoch: 16, Loss: 0.350260\n",
      "Epoch: 17, Loss: 0.128370\n",
      "Epoch: 18, Loss: 0.918424\n",
      "Epoch: 19, Loss: 0.169709\n",
      "Epoch: 20, Loss: 0.290795\n",
      "Epoch: 21, Loss: 0.206082\n",
      "Epoch: 22, Loss: 0.344824\n",
      "Epoch: 23, Loss: 0.158654\n",
      "Epoch: 24, Loss: 0.298542\n",
      "Epoch: 25, Loss: 0.266317\n",
      "Epoch: 26, Loss: 0.250415\n",
      "Epoch: 27, Loss: 0.058011\n",
      "Epoch: 28, Loss: 0.456119\n",
      "Epoch: 29, Loss: 0.108565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 30#SHOULD BE 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAN ONLY 30/100 EPOCHS OF ABOVE CODE SINCE ITS SLOW ON LAPTOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy on validation is 80% but on training is 99%, we are overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how many elements are in each tensor\n",
    "instance, we can call the numel method. Summing those gives us our total count.\n",
    "Depending on our use case, counting parameters might require us to check whether a\n",
    "parameter has requires_grad set to True, as well. We might want to differentiate the\n",
    "number of trainable parameters from the overall model size. Let’s take a look at what\n",
    "we have right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3737474"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this telling us? That our neural network won’t scale very well with the number\n",
    "of pixels. What if we had a 1,024 × 1,024 RGB image? That’s 3.1 million input values.\n",
    "Even abruptly going to 1,024 hidden features (which is not going to work for our clas\u0002sifier), we would have over 3 billion parameters. Using 32-bit floats, we’re already at 12\n",
    "GB of RAM, and we haven’t even hit the second layer, much less computed and stored\n",
    "the gradients. That’s just not going to fit on most present-day GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.7 The limits of going fully connected + conclusion\n",
    "In more\n",
    "technical terms, a fully connected network is not translation invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means a\n",
    "network that has been trained to recognize a Spitfire starting at position 4,4 will not\n",
    "be able to recognize the exact same Spitfire starting at position 8,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, at the end of this chapter, we have a dataset, a model, and a training loop, and\n",
    "our model learns. However, due to a mismatch between our problem and our network\n",
    "structure, we end up overfitting our training data, rather than learning the general\u0002ized features of what we want the model to detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we’re forced to\n",
    "use a lot of capacity for learning translated replicas if we want to hope to do well on\n",
    "the validation set. There has to be a better way, right?\n",
    " Of course, most such questions in a book like this are rhetorical. The solution to\n",
    "our current set of problems is to change our model to use convolutional layers. We’ll\n",
    "cover what that means in the next chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve also found a severe shortcoming of our model: we have been treating 2D\n",
    "images as 1D data. Also, we do not have a natural way to incorporate the translation\n",
    "invariance of our problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ture of image data to get much better results.9\n",
    " We could use what we have learned right away to process data without this translation\n",
    "invariance. For example, using it on tabular data or the time-series data we met in chap\u0002ter 4, we can probably do great things already. To some extent, it would also be possible\n",
    "to use it on text data that is appropriately represented.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
